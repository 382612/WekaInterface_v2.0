#BUTTON TEXT
button=Go to page

#NAVBAR TEXT
navbar.about=About
navbar.contact=Contact
navbar.home=Home

#HOME PAGE
home.subtitle=Explore the features of machine learning by using our easy to use interface.
home.title=Klessify

#ABOUT PAGE
about.title=About
about.subtitle=What's the point of this web application?
about.text=When starting out, learning about machine learling can be incredibly \
challenging. Being able to interpretate the results of creating \
a model requires significant knowledge about algorithms, algorithm \
parameters and mathematics. Besides that, the resources that are \
available during this steep learning curve aren't very beginner friendly. \
The weka software, however powerfull, is hard to use as a beginner \
due to it's unintuitive design and lack of information.<br> \
<br> \
To alleviate this beginners burden, we have made this web \
application to provide an easy workflow combined with easy \
access to information that is required to interpretate the \
results of your models and analysis.


#INFO PAGE
explorer.info=Explore weka and it's features.
information=What is machine learning?

infopage.basicinfoweka="Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
infopage.title=Weka Algortihms

#CONTACT PAGE
contact.title=Contact
contact.subtitle=Meet the team behind our Weka Interface.
student.email=Student email
student.names=Students
marijke.info=The Back End developer, responsible for all the moving parts on the server side that communicates with the HTML pages.
bart.info=Our gitmaster, responsible for version management, making \
sure that working in the same code project runs smoothly.
jelle.info=The Front End developer, creating the HTML pages that you see and\
small functionality on the pages.
michiel.info=The project supervisor, giving advice on how to tackle problems \
and providing sources of information.

#ALGORITHM INFORMATION PAGE
wekaAlgorithms.title=Weka Algortihms

ZeroR.information=The simplest of the rule based classifiers is the majority class classifier, called 0-R or\
ZeroR in Weka. The 0-R (zero rule) classifier takes a look at the target attribute and its possible\
values. It will always output the value that is most commonly found for the target attribute in the\
given dataset. 0-R as its names suggests; it does not include any rule that works on the non target\
attributes. So more specifically it predicts the mean (for a numeric type target attribute) or the mode\
(for a nominal type attribute).
OneR.information=This is also another simple rule based classifier. In this case only one rule is applied to the\
dataset, hence called 1R or OneR in Weka. The classifier will select one attribute and find the best\
classification rule based on it. Using the value of this attribute for a given instance, the rule will\
then predict the value of the target attribute. In other words, it uses the minimum-error attribute for\
prediction and discretizing numeric attributes. For more information see:\
R.C. Holte (1993). Very simple classification rules perform well on most commonly used datasets.
NaiveBayes.information=In statistics, naive Bayes classifiers are a family of simple "probabilistic classifiers" based on applying Bayes' theorem with strong (naïve) independence assumptions between the features (see Bayes classifier). They are among the simplest Bayesian network models, but coupled with kernel density estimation, they can achieve higher accuracy levels. Naïve Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression, which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.
IBK.information=In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric classification method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in data set. The output depends on whether k-NN is used for classification or regression:\
\
In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.
J48.information=J4.8 is Weka implementation of decision tree algorithm C4.5. It is actually built upon ID3\
algorithm by adding features like pruning as well as allowing numeric attribute values.\
C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan. C4.5 is an extension of Quinlan's earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a statistical classifier. In 2011, authors of the Weka machine learning software described the C4.5 algorithm as "a landmark decision tree program that is probably the machine learning workhorse most widely used in practice to date"
WekaAlgorithms.subtitle=Open the dropdown menu's to learn more about each machine\
learning algorithm that we've implemented in this web application.


#FILE UPLOAD
form.file.select=Select...
form.select=Select the demo file: 
form.upload=Or upload your own file:
form.choose=Choose file
form.classifier.select=Select the classifier:

# PARAMETER EXPLAINATIONS
batchsize.explaination=The preferred number of instances to process\n\
if batch prediction is being performed.
debug.explaination=If set to yes, the classifier \n\
may output additional information.
donotcheckcap.explaination=If set, classifier cpapabilities ar\n\
not checked before classifier is built.\n\
This might increase the time it takes\n\
to build the classifier.
numdecimals.explaination=The number of decimal places \n\
to be used for the output of\n\
numbers in the model.
minbucket.explaination=The minimum bucket size used\n\
for discretizing numeric attributes\n\
into groups.
confactor.explaination=The confidence factor is used in pruning.\n\
The smaller the value the more pruning is incured.
minnumobj.explaination=The minimum number of instances\n\
per leaf of the tree.
numfolds.explaination=Determines the amount of data\n\
used for 'reduced-error' pruning.
pruned.explaination=Wether the resulting tree will be pruned
laplace.explaination=
knn.explaination=The number of neighbours to use.
nnsearch.explaination=The nearest neighbour search algorithm to use
crossval.explaination=Wether hold-one-out cross-validation\n\
is applied to select the best k value\n\
between 1 and the value specified as\n\
the KNN parameter.


# PARAMS
true=Yes
false=No
max.batchsize=Maximum Batchsize
debug.par=Debug
doNotCheck.par=Do not check capabilities
num.decimal.par=Number of decimal places
minBucketSize.par=Minimum bucketsize
confidenceFactor.par=Confidence factor
minNumObj.par=Minimum number of objects
numFold.par=Number of folds
laPlace.par=la Place estimator
pruned.par=Pruned
knn.par=K-nearest neighbour
crossValidate.par=Crossvalidate
nnsearchAlgorithm.par=Nearest neighbour \
search algorithm

HistoryList=History List
# RESULTS PAGE
results.title=Results
button.return=Return
results.correct=Correctly Classified Instances              
results.perc.correct=Percentage Correctly Classified Instances   
results.incorrect=Incorrectly Classified Instances            
results.perc.incorrect=Percentage Incorrectly Classified Instances 
results.kappa=Kappa statistic                             
results.mean.abs=Mean absolute error                         
results.mean.squared=Root mean squared error                     
results.relative.abs=Relative absolute error                     
results.relative.squared=Root relative squared error                 
results.instances.num=Total Number of Instances                   
results.message=There are no results yet.
explorer.title=Weka Workbench
explorer.subtitle=Start looking into your dataset and create a classifier
404.title=Oops, this page doesn't exist
404.subtitle=The page that has been requested doesn't seem to exist.
500.title=Oops, something went wrong
500.subtitle=Something went wrong inside our server.
400.title=Oh no! A bad Request.
400.subtitle=Something went wrong communicating your input to the server.\
<br>Make sure you fill out forms before you press the 'submit' button.
information.title=Machine Learning
information.subtitle=Merriam Webster defines machine learning as follows:<br>\
<i>"The term machine learning (abbreviated ML) refers to the \
capability of a machine to improve its own performance. \
It does so by using a statistical model to make decisions \
and incorporating the result of each new trial into that \
model. In essence, the machine is programmed to learn \
through trial and error."</i><br>
numInstances.explaination=The number of instances used for building the model
correct.explaination=The number of correctly classified instances
pctCorrect.explaination=The percentage of the total amount of instances that have\
been classified correctly
incorrect.explaination=The amount of incorrectly classified instances
pctIncorrect.explaination=The percentage of the total amount of instances that have\
been classified incorrectly
kappa.explaination=The kappa statistic is a way of determining the reliability\
of a model. It is exressed as a value between 0 and 1. A value\
of 0 meaning it's not reliable and most likely happened by chance\
 and 1 meaning it's very reliable and probably isn't coincidental. 
meanAbsoluteError=In statistics, mean absolute error is a measure of difference between two continuous variables. Assume X and Y are variables of paired observations that express the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement.
rootMeanSquaredError=The root-mean-square deviation or root-mean-square error is a frequently used measure of the differences between values predicted by a model or an estimator and the values observed. The RMSD represents the square root of the second sample moment of the differences between predicted values and observed values or the quadratic mean of these differences.
relativeAbsoluteError=To be implemented
rootRelativeSquaredError=To be implemented




