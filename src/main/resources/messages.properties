#BUTTON TEXT
button=Go to page

#NAVBAR TEXT
navbar.about=About
navbar.contact=Contact
navbar.home=Home

#HOME PAGE
home.subtitle=Explore the features of machine learning by using our easy to use interface.
home.title=Klessify

#ABOUT PAGE
about.title=About
about.subtitle=\ 

#INFO PAGE
explorer.info=Explore weka and it's features.
information=What is machine learning?

infopage.basicinfoweka="Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
infopage.title=Weka Algortihms

#CONTACT PAGE
contact.title=Contact
contact.subtitle=Meet the team behind our Weka Interface.
student.email=Student email
student.names=Students
marijke.info=The Back End developer, responsible for all the moving parts on the server side that communicates with the HTML pages.
bart.info=Our gitmaster, responsible for version management, making \
sure that working in the same code project runs smoothly.
jelle.info=The Front End developer, creating the HTML pages that you see and\
small functionality on the pages.
michiel.info=The project supervisor, giving advice on how to tackle problems \
and providing sources of information.

#ALGORITHM INFORMATION PAGE
information.title=Weka Algortihms

ZeroR.information=The simplest of the rule based classifiers is the majority class classifier, called 0-R or\
ZeroR in Weka. The 0-R (zero rule) classifier takes a look at the target attribute and its possible\
values. It will always output the value that is most commonly found for the target attribute in the\
given dataset. 0-R as its names suggests; it does not include any rule that works on the non target\
attributes. So more specifically it predicts the mean (for a numeric type target attribute) or the mode\
(for a nominal type attribute).
OneR.information=This is also another simple rule based classifier. In this case only one rule is applied to the\
dataset, hence called 1R or OneR in Weka. The classifier will select one attribute and find the best\
classification rule based on it. Using the value of this attribute for a given instance, the rule will\
then predict the value of the target attribute. In other words, it uses the minimum-error attribute for\
prediction and discretizing numeric attributes. For more information see:\
R.C. Holte (1993). Very simple classification rules perform well on most commonly used datasets.
NaiveBayes.information=In statistics, naive Bayes classifiers are a family of simple "probabilistic classifiers" based on applying Bayes' theorem with strong (naïve) independence assumptions between the features (see Bayes classifier). They are among the simplest Bayesian network models, but coupled with kernel density estimation, they can achieve higher accuracy levels. Naïve Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression, which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.
IBK.information=In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric classification method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in data set. The output depends on whether k-NN is used for classification or regression:\
\
In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.
J48.information=J4.8 is Weka implementation of decision tree algorithm C4.5. It is actually built upon ID3\
algorithm by adding features like pruning as well as allowing numeric attribute values.\
C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan. C4.5 is an extension of Quinlan's earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a statistical classifier. In 2011, authors of the Weka machine learning software described the C4.5 algorithm as "a landmark decision tree program that is probably the machine learning workhorse most widely used in practice to date"
WekaAlgorithms.information="Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."


#FILE UPLOAD
form.file.select=Select...
form.select=Select the demo file: 
form.upload=Or upload your own file:
form.choose=Choose file
form.classifier.select=Select the classifier:

# PARAMETER EXPLAINATIONS
batchsize.explaination=The preferred number of instances to process\n\
if batch prediction is being performed.
debug.explaination=If set to yes, the classifier \n\
may output additional information.
donotcheckcap.explaination=If set, classifier cpapabilities ar\n\
not checked before classifier is built.\n\
This might increase the time it takes\n\
to build the classifier.
numdecimals.explaination=The number of decimal places \n\
to be used for the output of\n\
numbers in the model.
minbucket.explaination=The minimum bucket size used\n\
for discretizing numeric attributes\n\
into groups.
confactor.explaination=The confidence factor is used in pruning.\n\
The smaller the value the more pruning is incured.
minnumobj.explaination=The minimum number of instances\n\
per leaf of the tree.
numfolds.explaination=Determines the amount of data\n\
used for 'reduced-error' pruning.
pruned.explaination=Wether the resulting tree will be pruned
laplace.explaination=
knn.explaination=The number of neighbours to use.
nnsearch.explaination=The nearest neighbour search algorithm to use
crossval.explaination=Wether hold-one-out cross-validation\n\
is applied to select the best k value\n\
between 1 and the value specified as\n\
the KNN parameter.


# PARAMS
true=Yes
false=No
max.batchsize=Maximum Batchsize
debug.par=Debug
doNotCheck.par=Do not check capabilities
num.decimal.par=Number of decimal places
minBucketSize.par=Minimum bucketsize
confidenceFactor.par=Confidence factor
minNumObj.par=Minimum number of objects
numFold.par=Number of folds
laPlace.par=la Place estimator
pruned.par=Pruned
knn.par=K-nearest neighbour
crossValidate.par=Crossvalidate
nnsearchAlgorithm.par=Nearest neighbour \
search algorithm

HistoryList=History List
# RESULTS PAGE
results.title=Results
button.return=Return
results.correct=Correctly Classified Instances              
results.perc.correct=Percentage Correctly Classified Instances   
results.incorrect=Incorrectly Classified Instances            
results.perc.incorrect=Percentage Incorrectly Classified Instances 
results.kappa=Kappa statistic                             
results.mean.abs=Mean absolute error                         
results.mean.squared=Root mean squared error                     
results.relative.abs=Relative absolute error                     
results.relative.squared=Root relative squared error                 
results.instances.num=Total Number of Instances                   
explorer.title=Weka Workbench
explorer.subtitle=Start looking into your dataset and create a classifier


